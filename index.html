<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title></title>
    <meta name="author" content="Linjie  Luo">
    <meta name="description" content="">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="https://scholar.google.com/citations?user=fqubyX0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.linkedin.com/in/linjie-luo-981b8147" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
            <strong>Linjie Luo<strong>
          </strong></strong>
</h1>
          <h5 class="post-description"></h5>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/me.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="me.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I lead a team at ByteDance to develop cutting-edge generative human technology. Our goal is to create generative humans that are indistinguishable from real humans in appearance, expression and behavior with the scaling power of generative AI. We are always looking for great talents (both full-time &amp; intern) and feel free to contact me if interested: <code class="language-plaintext highlighter-rouge">linjie DOT luo AT gmail DOT com</code>.</p>

<p>My current research focus covers all aspects of the creation, driving and rendering of both virtual humans and stylized avatars. Some of my interests in the past also include 3D scene understanding, reconstruction and tracking. See <a href="/publications/">my publications</a> for more details.</p>

<p>I am also passionate about creating impact on people’s life through products. Over the years, I delivered numerous product features and tools in some of the most popular social platforms in the world, such as <a href="https://www.tiktok.com/discover/ai-self-tiktok" rel="external nofollow noopener" target="_blank">TikTok AI Self</a>, <a href="https://newsroom.tiktok.com/en-us/express-yourself-through-tiktok-avatars" rel="external nofollow noopener" target="_blank">TikTok Avatars</a>, Douyin <a href="http://www.xinhuanet.com/tech/2019-11/06/c_1125200607.htm" rel="external nofollow noopener" target="_blank">Landmark AR</a>, Snapchat <a href="https://lensstudio.snapchat.com/templates/landmarker/" rel="external nofollow noopener" target="_blank">Landmarkers </a>, <a href="https://lensstudio.snapchat.com/guides/general/tracking/marker-tracking" rel="external nofollow noopener" target="_blank">Marker Tracking</a>, <a href="https://support.snapchat.com/en-US/a/creative-tools" rel="external nofollow noopener" target="_blank">3D Stickers</a>.</p>

<p>Before joining ByteDance, I was a Lead Research Scientist at Snap Inc. Even before that, I worked as Research Scientist at <a href="https://research.adobe.com/" rel="external nofollow noopener" target="_blank">Adobe Research</a>. I obtained my PhD from <a href="http://www.princeton.edu/" rel="external nofollow noopener" target="_blank">Princeton University</a> <a href="http://www.cs.princeton.edu/" rel="external nofollow noopener" target="_blank">Computer Science Department</a> and my bachelor degree with honors from <a href="http://www.tsinghua.edu.cn/" rel="external nofollow noopener" target="_blank">Tsinghua University</a>.</p>

          </div>

          <!-- News -->
          <hr>
            <h2><a href="/news/" style="color: inherit;">news</a></h2>          <div class="news">
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row">Nov 10, 2024</th>
                  <td>
                    We released some early demos of <a href="https://byteaigc.github.io/X-Portrait2/" rel="external nofollow noopener" target="_blank">X-Portrait 2</a> pushing the expressiveness of portrait animation to the next level!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Mar 15, 2024</th>
                  <td>
                    <a href="/publications/">X-Portrait</a> is accepted to SIGGRAPH 2024.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 23, 2024</th>
                  <td>
                    <a href="/publications/">DiffPortrait3D</a> is accepted to CVPR 2024.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 20, 2023</th>
                  <td>
                    <a href="/publications/">OmniAvatar</a> and <a href="/publications/">PanoHead</a> are accepted to CVPR 2023.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Mar 10, 2022</th>
                  <td>
                    <a href="/publications/">AgileAvatar</a> is accepted to SIGGRAPH Asia 2022.

                  </td>
                </tr>
              </table>
            </div>
          </div>


          <!-- Selected papers -->
          <hr>
            <h2><a href="/publications/" style="color: inherit;">recent publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xportrait2-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xportrait2-thumbnail.gif"><div id="xportrait2-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xportrait2-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xportrait2-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xportrait2-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xportrait2-thumbnailgif');
      var modalImg = document.getElementById("xportrait2-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-XPortrait2" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-Portrait 2: Highly Expressive Portrait Animation</div>
    <!-- Author -->
    <div class="author">
      

      Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, <em>Linjie Luo</em>, Jinli Suo, and Yebin Liu</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Technical Demo</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a href="https://byteaigc.github.io/X-Portrait2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xportrait-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xportrait-thumbnail.gif"><div id="xportrait-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xportrait-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xportrait-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xportrait-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xportrait-thumbnailgif');
      var modalImg = document.getElementById("xportrait-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-SIG-XPortrait" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention</div>
    <!-- Author -->
    <div class="author">
      

      You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM SIGGRAPH</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1145/3641519.3657459" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/bytedance/X-Portrait" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://byteaigc.github.io/x-portrait/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-SIG-XPortrait</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, You and Xu, Hongyi and Song, Guoxian and Wang, Chao and Shi, Yichun and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400705250}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3641519.3657459}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3641519.3657459}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{115}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{ControlNet, generative model, portrait animation, stable diffusion, talking head}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="diffportrait3d-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/diffportrait3d-thumbnail.gif"><div id="diffportrait3d-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('diffportrait3d-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="diffportrait3d-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("diffportrait3d-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('diffportrait3d-thumbnailgif');
      var modalImg = document.getElementById("diffportrait3d-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-CVPR-DiffPortrait3D" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</div>
    <!-- Author -->
    <div class="author">
      

      Yuming Gu, Hongyi Xu, You Xie, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR) (Highlight)</em>, Jun 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_DiffPortrait3D_Controllable_Diffusion_for_Zero-Shot_Portrait_View_Synthesis_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/FreedomGu/DiffPortrait3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://freedomgu.github.io/DiffPortrait3D/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present DiffPortrait3D, a conditional diffusion model that is capable of synthesizing 3D-consistent photo-realistic novel views from as few as a single in-the-wild portrait. Specifically, given a single RGB input, we aim to synthesize plausible but consistent facial details rendered from novel camera views with retained both identity and facial expression. In lieu of time-consuming optimization and fine-tuning, our zero-shot method generalizes well to arbitrary face portraits with unposed camera views, extreme facial expressions, and diverse artistic depictions. At its core, we leverage the generative prior of 2D diffusion models pre-trained on large-scale image datasets as our rendering backbone, while the denoising is guided with disentangled attentive control of appearance and camera pose. To achieve this, we first inject the appearance context from the reference image into the self-attention layers of the frozen UNets. The rendering view is then manipulated with a novel conditional control module that interprets the camera pose by watching a condition image of a crossed subject from the same view. Furthermore, we insert a trainable cross-view attention module to enhance view consistency, which is further strengthened with a novel 3D-aware noise generation process during inference. We demonstrate state-of-the-art results both qualitatively and quantitatively on our challenging in-the-wild and multi-view benchmarks.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-CVPR-DiffPortrait3D</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Yuming and Xu, Hongyi and Xie, You and Song, Guoxian and Shi, Yichun and Chang, Di and Yang, Jing and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR) (Highlight)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="panohead-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/panohead-thumbnail.gif"><div id="panohead-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('panohead-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="panohead-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("panohead-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('panohead-thumbnailgif');
      var modalImg = document.getElementById("panohead-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-CVPR-OmniAvatar" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360</div>
    <!-- Author -->
    <div class="author">
      

      Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/23-CVPR-PanoHead.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://github.com/SizheAn/PanoHead" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://sizhean.github.io/panohead" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360° with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-CVPR-OmniAvatar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{An, Sizhe and Xu, Hongyi and Shi, Yichun and Song, Guoxian and Ogras, Umit and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="omniavatar-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/omniavatar-thumbnail.gif"><div id="omniavatar-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('omniavatar-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="omniavatar-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("omniavatar-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('omniavatar-thumbnailgif');
      var modalImg = document.getElementById("omniavatar-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-CVPR-OmniAvatas" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis</div>
    <!-- Author -->
    <div class="author">
      

      Hongyi Xu, Guoxian Song, Zihang Jiang, Jianfeng Zhang, Yichun Shi, Jing Liu, Wanchun Ma, Jiashi Feng, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/23-CVPR-OmniAvatar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://hongyixu37.github.io/omniavatar/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-CVPR-OmniAvatas</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Hongyi and Song, Guoxian and Jiang, Zihang and Zhang, Jianfeng and Shi, Yichun and Liu, Jing and Ma, Wanchun and Feng, Jiashi and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="agileavatar-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/agileavatar-thumbnail.jpg"><div id="agileavatar-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('agileavatar-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="agileavatar-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("agileavatar-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('agileavatar-thumbnailjpg');
      var modalImg = document.getElementById("agileavatar-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="22-SIGA-AgileAvatar" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging</div>
    <!-- Author -->
    <div class="author">
      

      Shen Sang, Tiancheng Zhi, Guoxian Song, Minghao Liu, Chunpong Lai, Jing Liu, Xiang Wen, James Davis, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>SIGGRAPH Asia</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/22-SIGA-AgileAvatar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://ssangx.github.io/projects/agileavatar/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Stylized 3D avatars have become increasingly prominent in our modern life. Creating these avatars manually usually involves laborious selection and adjustment of continuous and discrete parameters and is time-consuming for average users. Self-supervised approaches to automatically create 3D avatars from user selfies promise high quality with little annotation cost but fall short in application to stylized avatars due to a large style domain gap. We propose a novel self-supervised learning framework to create high-quality stylized 3D avatars with a mix of continuous and discrete parameters. Our cascaded domain bridging framework first leverages a modified portrait stylization approach to translate input selfies into stylized avatar renderings as the targets for desired 3D avatars. Next, we find the best parameters of the avatars to match the stylized avatar renderings through a differentiable imitator we train to mimic the avatar graphics engine. To ensure we can effectively optimize the discrete parameters, we adopt a cascaded relaxation-and-search pipeline. We use a human preference study to evaluate how well our method preserves user identity compared to previous work as well as manual creation. Our results achieve much higher preference scores than previous work and close to those of manual creation. We also provide an ablation study to justify the design choices in our pipeline.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">22-SIGA-AgileAvatar</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sang, Shen and Zhi, Tiancheng and Song, Guoxian and Liu, Minghao and Lai, Chunpong and Liu, Jing and Wen, Xiang and Davis, James and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394703}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Avatar Creation, Human Stylization}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daegu, Republic of Korea}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SA '22}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="agilegan-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/agilegan-thumbnail.jpg"><div id="agilegan-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('agilegan-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="agilegan-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("agilegan-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('agilegan-thumbnailjpg');
      var modalImg = document.getElementById("agilegan-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="21-SIG-AgileGAN" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning</div>
    <!-- Author -->
    <div class="author">
      

      Guoxian Song, <em>Linjie Luo</em>, Jing Liu, Wan-Chun Ma, Chunpong Lai, Chuanxia Zheng, and Tat-Jen Cham</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH)</em>, Aug 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/21-SIG-AgileGAN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://guoxiansong.github.io/homepage/agilegan.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Portraiture as an art form has evolved from realistic depiction into a plethora of creative styles. While substantial progress has been made in automated stylization, generating high quality stylistic portraits is still a challenge, and even the recent popular Toonify suffers from several artifacts when used on real input images. Such StyleGAN-based methods have focused on finding the best latent inversion mapping for reconstructing input images; however, our key insight is that this does not lead to good generalization to different portrait styles. Hence we propose AgileGAN, a framework that can generate high quality stylistic portraits via inversion-consistent transfer learning. We introduce a novel hierarchical variational autoencoder to ensure the inverse mapped distribution conforms to the original latent Gaussian distribution, while augmenting the original space to a multi-resolution latent space so as to better encode different levels of detail. To better capture attributedependent stylization of facial features, we also present an attribute-aware generator and adopt an early stopping strategy to avoid overfitting small training datasets. Our approach provides greater agility in creating high quality and high resolution (1024×1024) portrait stylization models, requiring only a limited number of style exemplars (∼100) and short training time (∼1 hour). We collected several style datasets for evaluation including 3D cartoons, comics, oil paintings and celebrities. We show that we can achieve superior portrait stylization quality to previous state-of-the-art methods, with comparisons done qualitatively, quantitatively and through a perceptual user study. We also demonstrate two applications of our method, image editing and motion retargeting.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">21-SIG-AgileGAN</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Song, Guoxian and Luo, Linjie and Liu, Jing and Ma, Wan-Chun and Lai, Chunpong and Zheng, Chuanxia and Cham, Tat-Jen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
          </div>


          <!-- Social -->
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Linjie  Luo. Last updated: November 21, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

  </body>
</html>
