<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Linjie Luo</title>
    <meta name="author" content="Linjie  Luo">
    <meta name="description" content="">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://linjieluo.github.io/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="https://scholar.google.com/citations?user=fqubyX0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.linkedin.com/in/linjie-luo-981b8147" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
            <strong>Linjie Luo<strong>
          </strong></strong>
</h1>
          <h5 class="post-description"></h5>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/me.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="me.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I lead a team at ByteDance to develop cutting-edge generative human technology. Our goal is to create generative humans that are indistinguishable from real humans in appearance, expression and behavior with the scaling power of generative AI. We are always looking for great talents (both full-time &amp; intern) and feel free to contact me if interested: <code class="language-plaintext highlighter-rouge">linjie DOT luo AT gmail DOT com</code>.</p>

<p>My current research focus covers all aspects of the creation, driving and rendering of both virtual humans and stylized avatars. Some of my interests in the past also include 3D scene understanding, reconstruction and tracking. See <a href="/publications/">my publications</a> for more details.</p>

<p>I am also passionate about creating impact on people’s life through products. Over the years, I delivered numerous product features and tools in some of the most popular social platforms in the world, such as <a href="https://www.tiktok.com/discover/ai-self-tiktok" rel="external nofollow noopener" target="_blank">TikTok AI Self</a>, <a href="https://newsroom.tiktok.com/en-us/express-yourself-through-tiktok-avatars" rel="external nofollow noopener" target="_blank">TikTok Avatars</a>, Douyin <a href="http://www.xinhuanet.com/tech/2019-11/06/c_1125200607.htm" rel="external nofollow noopener" target="_blank">Landmark AR</a>, Snapchat <a href="https://lensstudio.snapchat.com/templates/landmarker/" rel="external nofollow noopener" target="_blank">Landmarkers </a>, <a href="https://lensstudio.snapchat.com/guides/general/tracking/marker-tracking" rel="external nofollow noopener" target="_blank">Marker Tracking</a>, <a href="https://support.snapchat.com/en-US/a/creative-tools" rel="external nofollow noopener" target="_blank">3D Stickers</a>.</p>

<p>Before joining ByteDance, I was a Lead Research Scientist at Snap Inc. Even before that, I worked as Research Scientist at <a href="https://research.adobe.com/" rel="external nofollow noopener" target="_blank">Adobe Research</a>. I obtained my PhD from <a href="http://www.princeton.edu/" rel="external nofollow noopener" target="_blank">Princeton University</a> <a href="http://www.cs.princeton.edu/" rel="external nofollow noopener" target="_blank">Computer Science Department</a> and my bachelor degree with honors from <a href="http://www.tsinghua.edu.cn/" rel="external nofollow noopener" target="_blank">Tsinghua University</a>.</p>

          </div>

          <!-- News -->
          <hr>
            <h2><a href="/news/" style="color: inherit;">news</a></h2>          <div class="news">
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row">Mar 10, 2025</th>
                  <td>
                    <a href="/publications/">X-NeMo</a> (the technical paper of <a href="https://byteaigc.github.io/X-Portrait2/" rel="external nofollow noopener" target="_blank">X-Portrait 2</a>) is accepted to ICLR 2025.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 10, 2025</th>
                  <td>
                    <a href="/publications/">ID-Patch</a>, <a href="/publications/">COAP</a> and <a href="/publications/">X-Dyna</a> are accepted to CVPR 2025.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Nov 10, 2024</th>
                  <td>
                    We released some early demos of <a href="https://byteaigc.github.io/X-Portrait2/" rel="external nofollow noopener" target="_blank">X-Portrait 2</a> pushing the expressiveness of portrait animation to the next level!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Mar 15, 2024</th>
                  <td>
                    <a href="/publications/">X-Portrait</a> is accepted to SIGGRAPH 2024.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 23, 2024</th>
                  <td>
                    <a href="/publications/">DiffPortrait3D</a> is accepted to CVPR 2024.

                  </td>
                </tr>
              </table>
            </div>
          </div>


          <!-- Selected papers -->
          <hr>
            <h2><a href="/publications/" style="color: inherit;">recent publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="idpatch-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/idpatch-thumbnail.jpg"><div id="idpatch-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('idpatch-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="idpatch-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("idpatch-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('idpatch-thumbnailjpg');
      var modalImg = document.getElementById("idpatch-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="25-IDPatch" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">ID-Patch: Robust ID Association for Group Photo Personalization</div>
    <!-- Author -->
    <div class="author">
      

      Yimeng Zhang, Tiancheng Zhi, Jing Liu, Shen Sang, Liming Jiang, Qing Yan, Sijia Liu, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/bytedance/ID-Patch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://byteaigc.github.io/ID-Patch/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The ability to synthesize personalized group photos and specify the positions of each identity offers immense creative potential. While such imagery can be visually appealing, it presents significant challenges for existing technologies. A persistent issue is identity (ID) leakage, where injected facial features interfere with one another, resulting in low face resemblance, incorrect positioning, and visual artifacts. Existing methods suffer from limitations such as the reliance on segmentation models, increased runtime, or a high probability of ID leakage. To address these challenges, we propose ID-Patch, a novel method that provides robust association between identities and 2D positions. Our approach generates an ID patch and ID embeddings from the same facial features: the ID patch is positioned on the conditional image for precise spatial control, while the ID embeddings integrate with text embeddings to ensure high resemblance. Experimental results demonstrate that ID-Patch surpasses baseline methods across metrics, such as face ID resemblance, ID-position association accuracy, and generation efficiency.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">25-IDPatch</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yimeng and Zhi, Tiancheng and Liu, Jing and Sang, Shen and Jiang, Liming and Yan, Qing and Liu, Sijia and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ID-Patch: Robust ID Association for Group Photo Personalization}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xdyna-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xdyna-thumbnail.gif"><div id="xdyna-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xdyna-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xdyna-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xdyna-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xdyna-thumbnailgif');
      var modalImg = document.getElementById("xdyna-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="25-XDyna" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-Dyna: Expressive Dynamic Human Image Animation</div>
    <!-- Author -->
    <div class="author">
      

      Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Chenxu Zhang Shengqu Cai, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, <em>Linjie Luo</em>, Gordon Wetzstein, and Mohammad Soleymani</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/bytedance/X-Dyna" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://x-dyna.github.io/xdyna.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key factors underlying the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">25-XDyna</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chang, Di and Xu, Hongyi and Xie, You and Gao, Yipeng and Kuang, Zhengfei and Shengqu Cai, Chenxu Zhang and Song, Guoxian and Wang, Chao and Shi, Yichun and Chen, Zeyuan and Zhou, Shijie and Luo, Linjie and Wetzstein, Gordon and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-Dyna: Expressive Dynamic Human Image Animation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="coap-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/coap-thumbnail.jpg"><div id="coap-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('coap-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="coap-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("coap-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('coap-thumbnailjpg');
      var modalImg = document.getElementById("coap-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="25-COAP" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection</div>
    <!-- Author -->
    <div class="author">
      

      Jinqi Xiao, Shen Sang, Tiancheng Zhi, Jing Liu, Qing Yan, <em>Linjie Luo</em>, and Bo Yuan</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/bytedance/coap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://byteaigc.github.io/coap/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Training large-scale neural networks in vision, and multimodal domains demands substantial memory resources, primarily due to the storage of optimizer states. While LoRA, a popular parameter-efficient method, reduces memory usage, it often suffers from suboptimal performance due to the constraints of low-rank updates. Low-rank gradient projection methods (e.g., GaLore, Flora) reduce optimizer memory by projecting gradients and moment estimates into low-rank spaces via singular value decomposition or random projection. However, they fail to account for inter-projection correlation, causing performance degradation, and their projection strategies often incur high computational costs. In this paper, we present COAP (COrrelation-Aware Gradient Projection), a memory-efficient method that minimizes computational overhead while maintaining training performance. Evaluated across various vision, language, and multimodal tasks, COAP outperforms existing methods in both training speed and model performance. For LLaMA-1B, it reduces optimizer memory by 61% with only 2% additional time cost, achieving the same PPL as AdamW. With 8-bit quantization, COAP cuts optimizer memory by 81% and achieves 4x speedup over GaLore for LLaVA-v1.5-7B fine-tuning, while delivering higher accuracy.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">25-COAP</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xiao, Jinqi and Sang, Shen and Zhi, Tiancheng and Liu, Jing and Yan, Qing and Luo, Linjie and Yuan, Bo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xportrait2-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xportrait2-thumbnail.gif"><div id="xportrait2-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xportrait2-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xportrait2-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xportrait2-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xportrait2-thumbnailgif');
      var modalImg = document.getElementById("xportrait2-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="25-XNeMo" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention</div>
    <!-- Author -->
    <div class="author">
      

      Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, <em>Linjie Luo</em>, Jinli Suo, and Yebin Liu</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>International Conference on Learning Representations(ICLR)</em>, 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://byteaigc.github.io/X-Portrait2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the limitations in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on any pre-trained motion detectors. We further disentangle motion latents from identity cues with enhanced expressiveness by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention instead of additive spatial guidance, our design effectively eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models will be available for research.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">25-XNeMo</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Xiaochen and Xu, Hongyi and Song, Guoxian and Xie, You and Zhang, Chenxu and Li, Xiu and Luo, Linjie and Suo, Jinli and Liu, Yebin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations(ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xportrait-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xportrait-thumbnail.gif"><div id="xportrait-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xportrait-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xportrait-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xportrait-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xportrait-thumbnailgif');
      var modalImg = document.getElementById("xportrait-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-SIG-XPortrait" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention</div>
    <!-- Author -->
    <div class="author">
      

      You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM SIGGRAPH</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1145/3641519.3657459" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/bytedance/X-Portrait" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://byteaigc.github.io/x-portrait/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-SIG-XPortrait</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, You and Xu, Hongyi and Song, Guoxian and Wang, Chao and Shi, Yichun and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400705250}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3641519.3657459}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3641519.3657459}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{115}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{ControlNet, generative model, portrait animation, stable diffusion, talking head}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="diffportrait3d-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/diffportrait3d-thumbnail.gif"><div id="diffportrait3d-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('diffportrait3d-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="diffportrait3d-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("diffportrait3d-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('diffportrait3d-thumbnailgif');
      var modalImg = document.getElementById("diffportrait3d-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-CVPR-DiffPortrait3D" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</div>
    <!-- Author -->
    <div class="author">
      

      Yuming Gu, Hongyi Xu, You Xie, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR) (Highlight)</em>, Jun 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_DiffPortrait3D_Controllable_Diffusion_for_Zero-Shot_Portrait_View_Synthesis_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/FreedomGu/DiffPortrait3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://freedomgu.github.io/DiffPortrait3D/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present DiffPortrait3D, a conditional diffusion model that is capable of synthesizing 3D-consistent photo-realistic novel views from as few as a single in-the-wild portrait. Specifically, given a single RGB input, we aim to synthesize plausible but consistent facial details rendered from novel camera views with retained both identity and facial expression. In lieu of time-consuming optimization and fine-tuning, our zero-shot method generalizes well to arbitrary face portraits with unposed camera views, extreme facial expressions, and diverse artistic depictions. At its core, we leverage the generative prior of 2D diffusion models pre-trained on large-scale image datasets as our rendering backbone, while the denoising is guided with disentangled attentive control of appearance and camera pose. To achieve this, we first inject the appearance context from the reference image into the self-attention layers of the frozen UNets. The rendering view is then manipulated with a novel conditional control module that interprets the camera pose by watching a condition image of a crossed subject from the same view. Furthermore, we insert a trainable cross-view attention module to enhance view consistency, which is further strengthened with a novel 3D-aware noise generation process during inference. We demonstrate state-of-the-art results both qualitatively and quantitatively on our challenging in-the-wild and multi-view benchmarks.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-CVPR-DiffPortrait3D</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Yuming and Xu, Hongyi and Xie, You and Song, Guoxian and Shi, Yichun and Chang, Di and Yang, Jing and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR) (Highlight)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="panohead-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/panohead-thumbnail.gif"><div id="panohead-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('panohead-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="panohead-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("panohead-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('panohead-thumbnailgif');
      var modalImg = document.getElementById("panohead-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-CVPR-OmniAvatar" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360</div>
    <!-- Author -->
    <div class="author">
      

      Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/23-CVPR-PanoHead.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://github.com/SizheAn/PanoHead" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://sizhean.github.io/panohead" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360° with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-CVPR-OmniAvatar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{An, Sizhe and Xu, Hongyi and Shi, Yichun and Song, Guoxian and Ogras, Umit and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="omniavatar-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/omniavatar-thumbnail.gif"><div id="omniavatar-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('omniavatar-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="omniavatar-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("omniavatar-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('omniavatar-thumbnailgif');
      var modalImg = document.getElementById("omniavatar-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-CVPR-OmniAvatas" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis</div>
    <!-- Author -->
    <div class="author">
      

      Hongyi Xu, Guoxian Song, Zihang Jiang, Jianfeng Zhang, Yichun Shi, Jing Liu, Wanchun Ma, Jiashi Feng, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/23-CVPR-OmniAvatar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://hongyixu37.github.io/omniavatar/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-CVPR-OmniAvatas</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Hongyi and Song, Guoxian and Jiang, Zihang and Zhang, Jianfeng and Shi, Yichun and Liu, Jing and Ma, Wanchun and Feng, Jiashi and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="agileavatar-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/agileavatar-thumbnail.jpg"><div id="agileavatar-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('agileavatar-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="agileavatar-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("agileavatar-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('agileavatar-thumbnailjpg');
      var modalImg = document.getElementById("agileavatar-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="22-SIGA-AgileAvatar" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging</div>
    <!-- Author -->
    <div class="author">
      

      Shen Sang, Tiancheng Zhi, Guoxian Song, Minghao Liu, Chunpong Lai, Jing Liu, Xiang Wen, James Davis, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>SIGGRAPH Asia</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/22-SIGA-AgileAvatar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://ssangx.github.io/projects/agileavatar/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Stylized 3D avatars have become increasingly prominent in our modern life. Creating these avatars manually usually involves laborious selection and adjustment of continuous and discrete parameters and is time-consuming for average users. Self-supervised approaches to automatically create 3D avatars from user selfies promise high quality with little annotation cost but fall short in application to stylized avatars due to a large style domain gap. We propose a novel self-supervised learning framework to create high-quality stylized 3D avatars with a mix of continuous and discrete parameters. Our cascaded domain bridging framework first leverages a modified portrait stylization approach to translate input selfies into stylized avatar renderings as the targets for desired 3D avatars. Next, we find the best parameters of the avatars to match the stylized avatar renderings through a differentiable imitator we train to mimic the avatar graphics engine. To ensure we can effectively optimize the discrete parameters, we adopt a cascaded relaxation-and-search pipeline. We use a human preference study to evaluate how well our method preserves user identity compared to previous work as well as manual creation. Our results achieve much higher preference scores than previous work and close to those of manual creation. We also provide an ablation study to justify the design choices in our pipeline.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">22-SIGA-AgileAvatar</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sang, Shen and Zhi, Tiancheng and Song, Guoxian and Liu, Minghao and Lai, Chunpong and Liu, Jing and Wen, Xiang and Davis, James and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394703}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Avatar Creation, Human Stylization}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daegu, Republic of Korea}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SA '22}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="agilegan-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/agilegan-thumbnail.jpg"><div id="agilegan-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('agilegan-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="agilegan-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("agilegan-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('agilegan-thumbnailjpg');
      var modalImg = document.getElementById("agilegan-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="21-SIG-AgileGAN" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning</div>
    <!-- Author -->
    <div class="author">
      

      Guoxian Song, <em>Linjie Luo</em>, Jing Liu, Wan-Chun Ma, Chunpong Lai, Chuanxia Zheng, and Tat-Jen Cham</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH)</em>, Aug 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/21-SIG-AgileGAN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://guoxiansong.github.io/homepage/agilegan.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Portraiture as an art form has evolved from realistic depiction into a plethora of creative styles. While substantial progress has been made in automated stylization, generating high quality stylistic portraits is still a challenge, and even the recent popular Toonify suffers from several artifacts when used on real input images. Such StyleGAN-based methods have focused on finding the best latent inversion mapping for reconstructing input images; however, our key insight is that this does not lead to good generalization to different portrait styles. Hence we propose AgileGAN, a framework that can generate high quality stylistic portraits via inversion-consistent transfer learning. We introduce a novel hierarchical variational autoencoder to ensure the inverse mapped distribution conforms to the original latent Gaussian distribution, while augmenting the original space to a multi-resolution latent space so as to better encode different levels of detail. To better capture attributedependent stylization of facial features, we also present an attribute-aware generator and adopt an early stopping strategy to avoid overfitting small training datasets. Our approach provides greater agility in creating high quality and high resolution (1024×1024) portrait stylization models, requiring only a limited number of style exemplars (∼100) and short training time (∼1 hour). We collected several style datasets for evaluation including 3D cartoons, comics, oil paintings and celebrities. We show that we can achieve superior portrait stylization quality to previous state-of-the-art methods, with comparisons done qualitatively, quantitatively and through a perceptual user study. We also demonstrate two applications of our method, image editing and motion retargeting.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">21-SIG-AgileGAN</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Song, Guoxian and Luo, Linjie and Liu, Jing and Ma, Wan-Chun and Lai, Chunpong and Zheng, Chuanxia and Cham, Tat-Jen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
          </div>


          <!-- Social -->
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Linjie  Luo. Last updated: June 12, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

  </body>
</html>
