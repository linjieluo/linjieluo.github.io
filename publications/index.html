<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Linjie Luo</title>
    <meta name="author" content="Linjie  Luo">
    <meta name="description" content="publications">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/publications/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">
            <span class="font-weight-bold">Linjie Luo</span>
          </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- publication.html -->

<div class="post">
    <header class="post-header">
        <h1 class="post-title">publications</h1>
    </header>
    <article>
      <div class="publications">
        <h2 class="year">2024</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xportrait2-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xportrait2-thumbnail.gif"><div id="xportrait2-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xportrait2-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xportrait2-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xportrait2-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xportrait2-thumbnailgif');
      var modalImg = document.getElementById("xportrait2-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-XPortrait2" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-Portrait 2: Highly Expressive Portrait Animation</div>
    <!-- Author -->
    <div class="author">
      

      Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, <em>Linjie Luo</em>, Jinli Suo, and Yebin Liu</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Technical Demo</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a href="https://byteaigc.github.io/X-Portrait2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="xportrait-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/xportrait-thumbnail.gif"><div id="xportrait-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('xportrait-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="xportrait-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("xportrait-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('xportrait-thumbnailgif');
      var modalImg = document.getElementById("xportrait-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-SIG-XPortrait" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention</div>
    <!-- Author -->
    <div class="author">
      

      You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM SIGGRAPH</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.1145/3641519.3657459" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/bytedance/X-Portrait" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://byteaigc.github.io/x-portrait/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-SIG-XPortrait</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, You and Xu, Hongyi and Song, Guoxian and Wang, Chao and Shi, Yichun and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400705250}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3641519.3657459}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3641519.3657459}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{115}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{ControlNet, generative model, portrait animation, stable diffusion, talking head}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="picformer-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/picformer-thumbnail.jpg"><div id="picformer-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('picformer-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="picformer-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("picformer-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('picformer-thumbnailjpg');
      var modalImg = document.getElementById("picformer-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-TPAMI-PICFormer" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Bridging Global Context Interactions for High-Fidelity Pluralistic Image Completion</div>
    <!-- Author -->
    <div class="author">
      

      Chuanxia Zheng, Guoxian Song, Tat-Jen Cham, Jianfei Cai, <em>Linjie Luo</em>, and Dinh Phung</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/10535740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We introduce PICFormer, a novel framework for P luralistic I mage C ompletion using a trans Former based architecture, that achieves both high quality and diversity at a much faster inference speed. Our key contribution is to introduce a code-shared codebook learning using a restrictive CNN on small and non-overlapping receptive fields (RFs) for the local visible token representation. This results in a compact yet expressive discrete representation, facilitating efficient modeling of global visible context relations by the transformer. Unlike the prevailing autoregressive approaches, we proposed to sample all tokens simultaneously, leading to more than 100× faster inference speed. To enhance appearance consistency between visible and generated regions, we further propose a novel attention-aware layer (AAL), designed to better exploit distantly related high-frequency features. Through extensive experiments, we demonstrate that the PICFormer efficiently learns semantically-rich discrete codes, resulting in significantly improved image quality. Moreover, our diverse image completion framework surpasses State-of-the-Art methods on multiple image completion datasets.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-TPAMI-PICFormer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zheng, Chuanxia and Song, Guoxian and Cham, Tat-Jen and Cai, Jianfei and Luo, Linjie and Phung, Dinh}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bridging Global Context Interactions for High-Fidelity Pluralistic Image Completion}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8320-8333}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="diffportrait3d-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/diffportrait3d-thumbnail.gif"><div id="diffportrait3d-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('diffportrait3d-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="diffportrait3d-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("diffportrait3d-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('diffportrait3d-thumbnailgif');
      var modalImg = document.getElementById("diffportrait3d-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="24-CVPR-DiffPortrait3D" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</div>
    <!-- Author -->
    <div class="author">
      

      Yuming Gu, Hongyi Xu, You Xie, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR) (Highlight)</em>, Jun 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gu_DiffPortrait3D_Controllable_Diffusion_for_Zero-Shot_Portrait_View_Synthesis_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/FreedomGu/DiffPortrait3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://freedomgu.github.io/DiffPortrait3D/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present DiffPortrait3D, a conditional diffusion model that is capable of synthesizing 3D-consistent photo-realistic novel views from as few as a single in-the-wild portrait. Specifically, given a single RGB input, we aim to synthesize plausible but consistent facial details rendered from novel camera views with retained both identity and facial expression. In lieu of time-consuming optimization and fine-tuning, our zero-shot method generalizes well to arbitrary face portraits with unposed camera views, extreme facial expressions, and diverse artistic depictions. At its core, we leverage the generative prior of 2D diffusion models pre-trained on large-scale image datasets as our rendering backbone, while the denoising is guided with disentangled attentive control of appearance and camera pose. To achieve this, we first inject the appearance context from the reference image into the self-attention layers of the frozen UNets. The rendering view is then manipulated with a novel conditional control module that interprets the camera pose by watching a condition image of a crossed subject from the same view. Furthermore, we insert a trainable cross-view attention module to enhance view consistency, which is further strengthened with a novel 3D-aware noise generation process during inference. We demonstrate state-of-the-art results both qualitatively and quantitatively on our challenging in-the-wild and multi-view benchmarks.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">24-CVPR-DiffPortrait3D</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Yuming and Xu, Hongyi and Xie, You and Song, Guoxian and Shi, Yichun and Chang, Di and Yang, Jing and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR) (Highlight)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2023</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="hvtrpp-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/hvtrpp-thumbnail.jpg"><div id="hvtrpp-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('hvtrpp-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="hvtrpp-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("hvtrpp-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('hvtrpp-thumbnailjpg');
      var modalImg = document.getElementById("hvtrpp-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-TVCG-HVTR++" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">HVTR++: Image and Pose Driven Human Avatars using Hybrid Volumetric Textural Rendering</div>
    <!-- Author -->
    <div class="author">
      

      Tao Hu, Hongyi Xu, <em>Linjie Luo</em>, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and Matthias Zwicker</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Transactions on Visualization and Computer Graphics (TVCG)</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://dl.acm.org/doi/10.1109/TVCG.2023.3297721" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://github.com/TaoHuUMD/SurMo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://taohuumd.github.io/projects/hvtrpp/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Recent neural rendering methods have made great progress in generating photorealistic human avatars. However, these methods are generally conditioned only on low-dimensional driving signals (e.g., body poses), which are insufficient to encode the complete appearance of a clothed human. Hence they fail to generate faithful details. To address this problem, we exploit driving view images (e.g., in telepresence systems) as additional inputs. We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural Rendering (HVTR++), which synthesizes 3D human avatars from arbitrary driving poses and views while staying faithful to appearance details efficiently and at high quality. First, we learn to encode the driving signals of pose and view image on a dense UV manifold of the human body surface and extract UV-aligned features, preserving the structure of a skeleton-based parametric model. To handle complicated motions (e.g., self-occlusions), we then leverage the UV-aligned features to construct a 3D volumetric representation based on a dynamic neural radiance field. While this allows us to represent 3D geometry with changing topology, volumetric rendering is computationally heavy. Hence we employ only a rough volumetric representation using a pose- and image-conditioned downsampled neural radiance field (PID-NeRF), which we can render efficiently at low resolutions. In addition, we learn 2D textural features that are fused with rendered volumetric features in image space. The key advantage of our approach is that we can then convert the fused features into a high-resolution, high-quality avatar by a fast GAN-based textural renderer. We demonstrate that hybrid rendering enables HVTR++ to handle complicated motions, render high-quality avatars under user-controlled poses/shapes, and most importantly, be efficient at inference time. Our experimental results also demonstrate state-of-the-art quantitative results.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-TVCG-HVTR++</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Tao and Xu, Hongyi and Luo, Linjie and Yu, Tao and Zheng, Zerong and Zhang, He and Liu, Yebin and Zwicker, Matthias}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics (TVCG)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HVTR++: Image and Pose Driven Human Avatars using Hybrid Volumetric Textural Rendering}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dl.acm.org/doi/10.1109/TVCG.2023.3297721}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="panohead-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/panohead-thumbnail.gif"><div id="panohead-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('panohead-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="panohead-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("panohead-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('panohead-thumbnailgif');
      var modalImg = document.getElementById("panohead-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-CVPR-OmniAvatar" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360</div>
    <!-- Author -->
    <div class="author">
      

      Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/23-CVPR-PanoHead.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://github.com/SizheAn/PanoHead" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://sizhean.github.io/panohead" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360° with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-CVPR-OmniAvatar</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{An, Sizhe and Xu, Hongyi and Shi, Yichun and Song, Guoxian and Ogras, Umit and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="omniavatar-thumbnailgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/omniavatar-thumbnail.gif"><div id="omniavatar-thumbnailgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('omniavatar-thumbnailgif-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="omniavatar-thumbnailgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("omniavatar-thumbnailgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('omniavatar-thumbnailgif');
      var modalImg = document.getElementById("omniavatar-thumbnailgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="23-CVPR-OmniAvatas" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis</div>
    <!-- Author -->
    <div class="author">
      

      Hongyi Xu, Guoxian Song, Zihang Jiang, Jianfeng Zhang, Yichun Shi, Jing Liu, Wanchun Ma, Jiashi Feng, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/23-CVPR-OmniAvatar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://hongyixu37.github.io/omniavatar/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">23-CVPR-OmniAvatas</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Hongyi and Song, Guoxian and Jiang, Zihang and Zhang, Jianfeng and Shi, Yichun and Liu, Jing and Ma, Wanchun and Feng, Jiashi and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2022</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="agileavatar-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/agileavatar-thumbnail.jpg"><div id="agileavatar-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('agileavatar-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="agileavatar-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("agileavatar-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('agileavatar-thumbnailjpg');
      var modalImg = document.getElementById("agileavatar-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="22-SIGA-AgileAvatar" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging</div>
    <!-- Author -->
    <div class="author">
      

      Shen Sang, Tiancheng Zhi, Guoxian Song, Minghao Liu, Chunpong Lai, Jing Liu, Xiang Wen, James Davis, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>SIGGRAPH Asia</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/22-SIGA-AgileAvatar.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://ssangx.github.io/projects/agileavatar/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Stylized 3D avatars have become increasingly prominent in our modern life. Creating these avatars manually usually involves laborious selection and adjustment of continuous and discrete parameters and is time-consuming for average users. Self-supervised approaches to automatically create 3D avatars from user selfies promise high quality with little annotation cost but fall short in application to stylized avatars due to a large style domain gap. We propose a novel self-supervised learning framework to create high-quality stylized 3D avatars with a mix of continuous and discrete parameters. Our cascaded domain bridging framework first leverages a modified portrait stylization approach to translate input selfies into stylized avatar renderings as the targets for desired 3D avatars. Next, we find the best parameters of the avatars to match the stylized avatar renderings through a differentiable imitator we train to mimic the avatar graphics engine. To ensure we can effectively optimize the discrete parameters, we adopt a cascaded relaxation-and-search pipeline. We use a human preference study to evaluate how well our method preserves user identity compared to previous work as well as manual creation. Our results achieve much higher preference scores than previous work and close to those of manual creation. We also provide an ablation study to justify the design choices in our pipeline.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">22-SIGA-AgileAvatar</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sang, Shen and Zhi, Tiancheng and Song, Guoxian and Liu, Minghao and Lai, Chunpong and Liu, Jing and Wen, Xiang and Davis, James and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AgileAvatar: Stylized 3D Avatar Creation via Cascaded Domain Bridging}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394703}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Avatar Creation, Human Stylization}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daegu, Republic of Korea}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SA '22}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li></ol>
        
        <h2 class="year">2021</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="agilegan-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/agilegan-thumbnail.jpg"><div id="agilegan-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('agilegan-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="agilegan-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("agilegan-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('agilegan-thumbnailjpg');
      var modalImg = document.getElementById("agilegan-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="21-SIG-AgileGAN" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning</div>
    <!-- Author -->
    <div class="author">
      

      Guoxian Song, <em>Linjie Luo</em>, Jing Liu, Wan-Chun Ma, Chunpong Lai, Chuanxia Zheng, and Tat-Jen Cham</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH)</em>, Aug 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/21-SIG-AgileGAN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://guoxiansong.github.io/homepage/agilegan.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Portraiture as an art form has evolved from realistic depiction into a plethora of creative styles. While substantial progress has been made in automated stylization, generating high quality stylistic portraits is still a challenge, and even the recent popular Toonify suffers from several artifacts when used on real input images. Such StyleGAN-based methods have focused on finding the best latent inversion mapping for reconstructing input images; however, our key insight is that this does not lead to good generalization to different portrait styles. Hence we propose AgileGAN, a framework that can generate high quality stylistic portraits via inversion-consistent transfer learning. We introduce a novel hierarchical variational autoencoder to ensure the inverse mapped distribution conforms to the original latent Gaussian distribution, while augmenting the original space to a multi-resolution latent space so as to better encode different levels of detail. To better capture attributedependent stylization of facial features, we also present an attribute-aware generator and adopt an early stopping strategy to avoid overfitting small training datasets. Our approach provides greater agility in creating high quality and high resolution (1024×1024) portrait stylization models, requiring only a limited number of style exemplars (∼100) and short training time (∼1 hour). We collected several style datasets for evaluation including 3D cartoons, comics, oil paintings and celebrities. We show that we can achieve superior portrait stylization quality to previous state-of-the-art methods, with comparisons done qualitatively, quantitatively and through a perceptual user study. We also demonstrate two applications of our method, image editing and motion retargeting.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">21-SIG-AgileGAN</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Song, Guoxian and Luo, Linjie and Liu, Jing and Ma, Wan-Chun and Lai, Chunpong and Zheng, Chuanxia and Cham, Tat-Jen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AgileGAN: Stylizing Portraits by Inversion-Consistent Transfer Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li></ol>
        
        <h2 class="year">2020</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="holicity-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/holicity-thumbnail.jpg"><div id="holicity-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('holicity-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="holicity-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("holicity-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('holicity-thumbnailjpg');
      var modalImg = document.getElementById("holicity-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="20-HoliCity" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">HoliCity: A City-Scale Data Platform for Learning Holistic 3D Structures</div>
    <!-- Author -->
    <div class="author">
      

      Yichao Zhou, Jingwei Huang, Xili Dai, <em>Linjie Luo</em>, Zhili Chen, and Yi Ma</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ICCV Workshop</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/20-HoliCity-ACityScaleDataPlatformForLearningHolistic3DStructures.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://holicity.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present HoliCity, a city-scale 3D dataset with rich structural information. Currently, this dataset has 6,300 real-world panoramas of resolution 13312 × 6656 that are accurately aligned with the CAD model of downtown London with an area of more than 20 km2, in which the median reprojection error of the alignment of an average image is less than half a degree. This dataset aims to be an all-in- one data platform for research of learning abstracted high-level holistic 3D structures that can be derived from city CAD models, e.g., corners, lines, wireframes, planes, and cuboids, with the ultimate goal of supporting real-world applications including city-scale reconstruction, localization, mapping, and augmented reality. The accurate alignment of the 3D CAD models and panoramas also benefits low-level 3D vision tasks such as surface normal estimation, as the surface normal extracted from previous LiDAR-based datasets is often noisy. We conduct experiments to demonstrate the applications of HoliCity, such as predicting surface segmentation, normal maps, depth maps, and vanishing points, as well as test the generalizability of methods trained on HoliCity and other related datasets. HoliCity is available at https://holicity.io.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">20-HoliCity</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Yichao and Huang, Jingwei and Dai, Xili and Luo, Linjie and Chen, Zhili and Ma, Yi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HoliCity: A City-Scale Data Platform for Learning Holistic 3D Structures}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICCV Workshop}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="dynocc-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/dynocc-thumbnail.jpg"><div id="dynocc-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('dynocc-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="dynocc-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("dynocc-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('dynocc-thumbnailjpg');
      var modalImg = document.getElementById("dynocc-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="20-3DV-DynOcc" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">DynOcc: Learning Single-View Depth from Dynamic Occlusion Cues</div>
    <!-- Author -->
    <div class="author">
      

      Yifan Wang, <em>Linjie Luo</em>, Xiaohui Shen, and Xing Mei</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>International Conference on 3D Vision (3DV)</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/20-3DV-DynOcc-LearningSingleViewDepthFromDynamicOcclusionCues.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Recently, significant progress has been made in single-view depth estimation thanks to increasingly large and diverse depth datasets. However, these datasets are largely limited to specific application domains (e.g. indoor, autonomous driving) or static in-the-wild scenes due to hard-ware constraints or technical limitations of 3D reconstruction. In this paper, we introduce the first depth dataset DynOcc consisting of dynamic in-the-wild scenes. Our approach leverages the occlusion cues in these dynamic scenes to infer depth relationships between points of selected video frames. To achieve accurate occlusion detection and depth order estimation, we employ a novel occlusion boundary detection, filtering and thinning scheme followed by a robust foreground/background classification method. In total our DynOcc dataset contains 22M depth pairs out of 91K frames from a diverse set of videos. Using our dataset we achieved state-of-the-art results measured in weighted human disagreement rate (WHDR). We also show that the inferred depth maps trained with DynOcc can preserve sharper depth boundaries.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">20-3DV-DynOcc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yifan and Luo, Linjie and Shen, Xiaohui and Mei, Xing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DynOcc: Learning Single-View Depth from Dynamic Occlusion Cues}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on 3D Vision (3DV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="scene-thumbnailpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/scene-thumbnail.png"><div id="scene-thumbnailpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('scene-thumbnailpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="scene-thumbnailpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("scene-thumbnailpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('scene-thumbnailpng');
      var modalImg = document.getElementById("scene-thumbnailpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="20-TOG-SceneGen" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Deep Generative Modeling for Scene Synthesis via Hybrid Representations</div>
    <!-- Author -->
    <div class="author">
      

      Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, <em>Linjie Luo</em>, Alexander Huth, Etienne Vouga, and Qixing Huang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics</em>, Apr 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/20-TOG-DeepGenerativeModelingForSceneSynthesisViaHybridRepresentations.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present a deep generative scene modeling technique for indoor environments. Our goal is to train a generative model using a feed-forward neural network that maps a prior distribution (e.g., a normal distribution) to the distribution of primary objects in indoor scenes. We introduce a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes. Moreover, our scene representation is applicable for 3D objects with different multiplicities (repetition counts), selected from a database. We show a principled way to train this model by combining discriminative losses for both a 3D object arrangement representation and a 2D image-based representation. We demonstrate the effectiveness of our scene representation and the network training method</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">20-TOG-SceneGen</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zaiwei and Yang, Zhenpei and Ma, Chongyang and Luo, Linjie and Huth, Alexander and Vouga, Etienne and Huang, Qixing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Generative Modeling for Scene Synthesis via Hybrid Representations}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2019</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="tbn-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/tbn-thumbnail.jpg"><div id="tbn-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('tbn-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="tbn-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("tbn-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('tbn-thumbnailjpg');
      var modalImg = document.getElementById("tbn-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="19-ICCV-TBN" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Transformable Bottleneck Networks</div>
    <!-- Author -->
    <div class="author">
      

      Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/19-ICCV-TransformableBottleneckNetworks.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://kyleolsz.github.io/TB-Networks/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose a novel approach to performing fine-grained 3D manipulation of image content via a convolutional neural network, which we call the Transformable Bottleneck Network (TBN). It applies given spatial transformations directly to a volumetric bottleneck within our encoder-bottleneck-decoder architecture. Multi-view supervision encourages the network to learn to spatially disentangle the feature space within the bottleneck. The resulting spatial structure can be manipulated with arbitrary spatial transformations. We demonstrate the efficacy of TBNs for novel view synthesis, achieving state-of-the-art results on a challenging benchmark. We demonstrate that the bottlenecks produced by networks trained for this task contain meaningful spatial structure that allows us to intuitively perform a variety of image manipulations in 3D, well beyond the rigid transformations seen during training. These manipulations include non-uniform scaling, non-rigid warping, and combining content from different images. Finally, we extract explicit 3D structure from the bottleneck, performing impressive 3D reconstruction from a single input image.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">19-ICCV-TBN</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Olszewski, Kyle and Tulyakov, Sergey and Woodford, Oliver and Li, Hao and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transformable Bottleneck Networks}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="kernelpose-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/kernelpose-thumbnail.jpg"><div id="kernelpose-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('kernelpose-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="kernelpose-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("kernelpose-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('kernelpose-thumbnailjpg');
      var modalImg = document.getElementById("kernelpose-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="19-ICCV-KernelPose" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Dynamic Kernel Distillation for Efficient Pose Estimation in Videos</div>
    <!-- Author -->
    <div class="author">
      

      Xuecheng Nie, Yuncheng Li, <em>Linjie Luo</em>, Ning Zhang, and Jiashi Feng</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Dynamic_Kernel_Distillation_for_Efficient_Pose_Estimation_in_Videos_ICCV_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">19-ICCV-KernelPose</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nie, Xuecheng and Li, Yuncheng and Luo, Linjie and Zhang, Ning and Feng, Jiashi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Kernel Distillation for Efficient Pose Estimation in Videos}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="extreme-pose-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/extreme-pose-thumbnail.jpg"><div id="extreme-pose-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('extreme-pose-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="extreme-pose-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("extreme-pose-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('extreme-pose-thumbnailjpg');
      var modalImg = document.getElementById("extreme-pose-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="19-CVPR-ExtremePose" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</div>
    <!-- Author -->
    <div class="author">
      

      Zhenpei Yang, Jeffrey Z.Pan, <em>Linjie Luo</em>, Xiaowei Zhou, Kristen Grauman, and Qixing Huang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/19-CVPR-ExtremeRelativePose.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Estimating the relative rigid pose between two RGB-D scans of the same underlying environment is a fundamental problem in computer vision, robotics, and computer graphics. Most existing approaches allow only limited relative pose changes since they require considerable overlap between the input scans. We introduce a novel approach that extends the scope to extreme relative poses, with little or even no overlap between the input scans. The key idea is to infer more complete scene information about the underlying environment and match on the completed scans. In particular, instead of only performing scene completion from each individual scan, our approach alternates between relative pose estimation and scene completion. This allows us to perform scene completion by utilizing information from both input scans at late iterations, resulting in better results for both scene completion and relative pose estimation. Experimental results on benchmark datasets show that our approach leads to considerable improvements over state-of- the-art approaches for relative pose estimation. In particular, our approach provides encouraging relative pose estimates even between non-overlapping scans.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">19-CVPR-ExtremePose</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Zhenpei and Z.Pan, Jeffrey and Luo, Linjie and Zhou, Xiaowei and Grauman, Kristen and Huang, Qixing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2018</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="stabilized-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/stabilized-thumbnail.jpg"><div id="stabilized-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('stabilized-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="stabilized-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("stabilized-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('stabilized-thumbnailjpg');
      var modalImg = document.getElementById("stabilized-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="18-SIG-Stabilized" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Stabilized Real-time Face Tracking via a Learned Dynamic Rigidity Prior</div>
    <!-- Author -->
    <div class="author">
      

      Chen Cao, Menglei Chai, Oliver Woodford, and <em>Linjie Luo</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Nov 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/18-SIGA-StabilizedRealtimeFaceTrackingViaALearnedDynamicRigidityPrior.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="/assets/pdf/18-SIGA-StabilizedRealtimeFaceTrackingViaALearnedDynamicRigidityPrior.mp4" class="btn btn-sm z-depth-0" role="button">Supp</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Despite the popularity of real-time monocular face tracking systems in many successful applications, one overlooked problem with these systems is rigid instability. It occurs when the input facial motion can be explained by either head pose change or facial expression change, creating ambiguities that often lead to jittery and unstable rigid head poses under large expressions. Existing rigid stabilization methods either employ a heavy anatomically-motivated approach that are unsuitable for real-time applications, or utilize heuristic-based rules that can be problematic under certain expressions. We propose the first rigid stabilization method for real-time monocular face tracking using a dynamic rigidity prior learned from realistic datasets. The prior is defined on a region-based face model and provides dynamic region-based adaptivity for rigid pose optimization during real-time performance. We introduce an effective offline training scheme to learn the dynamic rigidity prior by optimizing the convergence of the rigid pose optimization to the ground-truth poses in the training data. Our real-time face tracking system is an optimization framework that alternates between rigid pose optimization and expression optimization. To ensure tracking accuracy, we combine both robust, drift-free facial landmarks and dense optical flow into the optimization objectives. We evaluate our system extensively against state-of-the-art monocular face tracking systems and achieve significant improvement in tracking accuracy on the high-quality face tracking benchmark. Our system can improve facial-performance-based applications such as facial animation retargeting and virtual face makeup with accurate expression and stable pose. We further validate the dynamic rigidity prior by comparing it against other variants on the tracking accuracy.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">18-SIG-Stabilized</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Chen and Chai, Menglei and Woodford, Oliver and Luo, Linjie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stabilized Real-time Face Tracking via a Learned Dynamic Rigidity Prior}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="hairvae-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/hairvae-thumbnail.jpg"><div id="hairvae-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('hairvae-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="hairvae-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("hairvae-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('hairvae-thumbnailjpg');
      var modalImg = document.getElementById("hairvae-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="18-SIG-3DHairVAE" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">3D Hair Synthesis Using Volumetric Variational Autoencoders</div>
    <!-- Author -->
    <div class="author">
      

      Shunsuke Saito, Liwen Hu, Chongyang Ma, <em>Linjie Luo</em>, and Hao Li</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Nov 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/18-SIGA-3DHairSynthesisUsingVolumetricVAEs.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      <a href="https://vgl.ict.usc.edu/Research/3DHairSynthesis/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Recent advances in single-view 3D hair digitization have made the creation of high-quality CG characters scalable and accessible to end-users, enabling new forms of personalized VR and gaming experiences. To handle the complexity and variety of hair structures, most cutting-edge techniques rely on the successful retrieval of a particular hair model from a comprehensive hair database. Not only are the aforementioned data-driven methods storage intensive, but they are also prone to failure for highly unconstrained input images, exotic hairstyles, failed face detection. Instead of using a large collection of 3D hair models directly, we propose to represent the manifold of 3D hairstyles implicitly through a compact latent space of a volumetric variational autoencoder (VAE). This deep neural network is trained with volumetric orientation field representations of 3D hair models and can synthesize new hairstyles from a compressed code. To enable end-to-end 3D hair inference, we train an additional regression network to predict the codes in the VAE latent space from any input image. Strand-level hairstyles can then be generated from the predicted volumetric representation. Our fully automatic framework does not require any ad-hoc face fitting, intermediate classification and segmentation, or hairstyle database retrieval. Our hair synthesis approach is significantly more robust than and can handle a much wider variation of hairstyles than state-of-the-art data-driven hair modeling techniques w.r.t. challenging inputs, including photos that are low-resolution, over-exposed, or contain extreme head poses. The storage requirements are minimal and a 3D hair model can be produced from an image in a second. Our evaluations also show that successful reconstructions are possible from highly stylized cartoon images, non-human subjects, and pictures taken from the back of a person. Our approach is particularly well suited for continuous and plausible hair interpolation between very different hairstyles.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">18-SIG-3DHairVAE</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saito, Shunsuke and Hu, Liwen and Ma, Chongyang and Luo, Linjie and Li, Hao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="da-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/da-thumbnail.jpg"><div id="da-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('da-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="da-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("da-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('da-thumbnailjpg');
      var modalImg = document.getElementById("da-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="18-ECCV-3DKeypointDA" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency</div>
    <!-- Author -->
    <div class="author">
      

      Xingyi Zhou, Arjun Karpur, Chuang Gan, <em>Linjie Luo</em>, and Qixing Huang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>European Conference on Computer Vision (ECCV)</em>, 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/18-ECCV-UnsupervisedDomainAdaptationFor3DKeypointEstimationViaViewConsistency.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan/image. Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other. Such view consistency provides effective regularization for keypoint prediction on unlabeled instances. In addition, we introduce a geometric alignment term to regularize predictions in the target domain. The resulting loss function can be effectively optimized via alternating minimization. We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art general-purpose domain adaptation techniques.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">18-ECCV-3DKeypointDA</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Xingyi and Karpur, Arjun and Gan, Chuang and Luo, Linjie and Huang, Qixing}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="starmap-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/starmap-thumbnail.jpg"><div id="starmap-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('starmap-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="starmap-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("starmap-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('starmap-thumbnailjpg');
      var modalImg = document.getElementById("starmap-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="18-ECCV-StarMap" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">StarMap for Category-Agnostic Semantic Keypoint Representations in Canonical Object Views</div>
    <!-- Author -->
    <div class="author">
      

      Xingyi Zhou, Arjun Karpur, <em>Linjie Luo</em>, and Qixing Huang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>European Conference on Computer Vision (ECCV)</em>, 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Xingyi_Zhou_Category-Agnostic_Semantic_Keypoint_ECCV_2018_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Semantic keypoints provide concise abstractions for a variety of visual understanding tasks. Existing methods define semantic keypoints separately for each category with a fixed number of semantic labels.
  As a result, these keypoints are not suitable when objects have a varying number of parts, e.g. chairs with varying number of legs. We propose a category-agnostic keypoint representation encoded with their 3D locations in the canonical object views. Our intuition is that the 3D locations of the keypoints in canonical object views contain rich semantic and compositional information. Our representation thus consists of a single channel, multi-peak heatmap (StarMap) for all the keypoints and their corresponding features as 3D locations in the canonical object view (CanViewFeature) defined for each category. Not only is our representation flexible, but we also demonstrate competitive performance in keypoint detection and localization compared to category-specific state-of-the-art methods. Moreover, we show that when augmented with an additional depth channel (DepthMap) to lift the 2D keypoints to 3D,our representation can achieve state-of-the-art results in viewpoint estimation. Finally, we demonstrate that each individual component of our framework can be used on the task of human pose estimation to simplify the state-of-the-art architectures.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">18-ECCV-StarMap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StarMap for Category-Agnostic Semantic Keypoint Representations in Canonical Object Views}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhou, Xingyi and Karpur, Arjun and Luo, Linjie and Huang, Qixing}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="sparse-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/sparse-thumbnail.jpg"><div id="sparse-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('sparse-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="sparse-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("sparse-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('sparse-thumbnailjpg');
      var modalImg = document.getElementById("sparse-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="18-ECCV-VerySparseVol" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Deep Volumetric Video From Very Sparse Multi-View Performance Capture</div>
    <!-- Author -->
    <div class="author">
      

      Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing, Chloe LeGendre, Chongyang Ma, <em>Linjie Luo</em>, and Hao Li</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>European Conference on Computer Vision (ECCV)</em>, 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/18-ECCV-DeepVolumetricVideoFromVerySparseMultiViewPerformanceCapture.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present a deep learning-based volumetric capture approach for performance capture using a passive and highly sparse multi-view capture system. We focus on a template-free, per-frame 3D surface reconstruction from as few as three RGB sensors, where conventional visual hull or multi-view stereo methods would fail. State-of-the-art performance capture systems require either pre-scanned actors, large number of cameras or active sensors. We introduce a novel multi-view Convolutional Neural Network (CNN) that maps 2D images to a 3D volumetric field that encodes the probabilistic distribution of surface points of the captured subject. By querying the resulting field, we can instantiate the clothed human body at arbitrary resolutions. Our approach also scales to different numbers of input images, which yield increased reconstruction quality when more views are used. Though only trained on synthetic data, our network can generalize to real captured performances. Since high-quality temporal surface reconstructions are possible, our method is suitable for low-cost full body volumetric capture solutions for consumers, which are gaining popularity for VR and AR content creation. Experimental results demonstrate that our method is significantly more robust and accurate than existing techniques where only very sparse views are available.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">18-ECCV-VerySparseVol</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Volumetric Video From Very Sparse Multi-View Performance Capture}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Zeng and Li, Tianye and Chen, Weikai and Zhao, Yajie and Xing, Jun and LeGendre, Chloe and Ma, Chongyang and Luo, Linjie and Li, Hao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2017</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="autoscaler-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/autoscaler-thumbnail.jpg"><div id="autoscaler-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('autoscaler-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="autoscaler-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("autoscaler-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('autoscaler-thumbnailjpg');
      var modalImg = document.getElementById("autoscaler-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="17-BMVC-AutoScaler" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">AutoScaler: Scale-Attention Networks for Visual Correspondence</div>
    <!-- Author -->
    <div class="author">
      

      Shenlong Wang, <em>Linjie Luo</em>, Ning Zhang, and Li-Jia Li</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>BMVC</em>, 2017
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/16-BMVC-AutoScaler.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our architecture consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive sizes of equivalent receptive field over different scales of the input. The entire network can be trained end-to-end in a Siamese framework for visual correspondence tasks. Using the latest off-the-shelf architecture for the feature network, our method achieves competitive results compared to state-of-the-art methods on challenging optical flow and semantic matching bench- marks, including Sintel, KITTI and CUB-2011. We also show that our attention network alone can be applied to existing hand-crafted feature descriptors (e.g Daisy) and improve their performance on visual correspondence tasks. Finally, we illustrate how the scale- attention maps generated from the attention network are visually interpretable.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">17-BMVC-AutoScaler</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AutoScaler: Scale-Attention Networks for Visual Correspondence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Shenlong and Luo, Linjie and Zhang, Ning and Li, Li-Jia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{BMVC}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li></ol>
        
        <h2 class="year">2016</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="cutout-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/cutout-thumbnail.jpg"><div id="cutout-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('cutout-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="cutout-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("cutout-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('cutout-thumbnailjpg');
      var modalImg = document.getElementById("cutout-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="16-WACV-CustomExpr" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Customized expression recognition for performance-driven cutout character animation</div>
    <!-- Author -->
    <div class="author">
      

      Xiang Yu, Jianchao Yang, <em>Linjie Luo</em>, Wilmot Li, Jonathan Brandt, and Dimitris Metaxas</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, Mar 2016
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Performance-driven character animation enables users to create expressive results by performing the desired motion of the character with their face and/or body. How- ever, for cutout animations where continuous motion is combined with discrete artwork replacements, supporting a performance-driven workflow has some unique requirements. To trigger the appropriate artwork replacements, the system must reliably detect a wide range of customized facial expressions that are challenging for existing recognition methods, which focus on a few canonical expressions (e.g., angry, disgusted, scared, happy, sad and surprised). Also, real usage scenarios require the system to work in realtime with minimal training. In this paper, we propose a novel customized expression recognition technique that meets all of these requirements. We first use a set of handcrafted features combining geometric features derived from facial landmarks and patch-based appearance features through group sparsity- based facial component learning. To improve discrimination and generalization, these handcrafted features are integrated into a custom-designed Deep Convolutional Neural Network (CNN) structure trained from publicly available facial expression datasets. The combined features are fed to an online ensemble of SVMs designed for the few train- ing sample problem and performs in realtime. To improve temporal coherence, we also apply a Hidden Markov Mod- el (HMM) to smooth the recognition results. Our system achieves state-of-the-art performance on canonical expression datasets and promising results on our collected dataset of customized expressions.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">16-WACV-CustomExpr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Xiang and Yang, Jianchao and Luo, Linjie and Li, Wilmot and Brandt, Jonathan and Metaxas, Dimitris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Winter Conference on Applications of Computer Vision (WACV)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Customized expression recognition for performance-driven cutout character animation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-9}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li></ol>
        
        <h2 class="year">2015</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="relief-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/relief-thumbnail.jpg"><div id="relief-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('relief-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="relief-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("relief-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('relief-thumbnailjpg');
      var modalImg = document.getElementById("relief-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="15-SIGA-HairRelief" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">High-Quality Hair Modeling from A Single Portrait Photo</div>
    <!-- Author -->
    <div class="author">
      

      Menglei Chai, <em>Linjie Luo</em>, Kalyan Sunkavalli, Nathan Carr, Sunil Hadap, and Kun Zhou</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Nov 2015
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/15-SIGA-HighQualityHairModelingFromASinglePortraitPhoto.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose a novel system to reconstruct a high-quality hair depth map from a single portrait photo with minimal user input. We achieve this by combining depth cues such as occlusions, silhouettes, and shading, with a novel 3D helical structural prior for hair reconstruction. We fit a parametric morphable face model to the input photo and construct a base shape in the face, hair and body regions using occlusion and silhouette constraints. We then estimate the normals in the hair region via a Shape-from-Shading-based optimization that uses the lighting inferred from the face model and enforces an adaptive albedo prior that models the typical color and occlusion variations of hair. We introduce a 3D helical hair prior that captures the geometric structure of hair, and show that it can be robustly recovered from the input photo in an automatic manner. Our system combines the base shape, the normals estimated by Shape from Shading, and the 3D helical hair prior to reconstruct high-quality 3D hair models. Our single-image reconstruction closely matches the results of a state-of-the-art multi-view stereo applied on a multi-view stereo dataset. Our technique can reconstruct a wide variety of hairstyles ranging from short to long and from straight to messy, and we demonstrate the use of our 3D hair models for high-quality portrait relighting, novel view synthesis and 3D-printed portrait reliefs.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">15-SIGA-HairRelief</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chai, Menglei and Luo, Linjie and Sunkavalli, Kalyan and Carr, Nathan and Hadap, Sunil and Zhou, Kun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{High-Quality Hair Modeling from A Single Portrait Photo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="levelset-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/levelset-thumbnail.jpg"><div id="levelset-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('levelset-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="levelset-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("levelset-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('levelset-thumbnailjpg');
      var modalImg = document.getElementById("levelset-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="15-SIGA-LevelSetPrintable" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Level-Set-Based Partitioning and Packing Optimization of a Printable Model</div>
    <!-- Author -->
    <div class="author">
      

      Miaojun Yao, Zhili Chen, <em>Linjie Luo</em>, Rui Wang, and Huamin Wang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Nov 2015
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/15-SIGA-LevelSetBasedPartitioningAndPackingOptimizationOfAPrintableModel.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>As the 3D printing technology starts to revolutionize our daily life and the manufacturing industries, a critical problem is about to emerge: how can we find an automatic way to divide a 3D model into multiple printable pieces, so as to save the space, to reduce the printing time, or to make a large model printable by small printers. In this paper, we present a systematic study on the partitioning and packing of 3D models under the multi-phase level set framework. We first construct analysis tools to evaluate the qualities of a partitioning using six metrics: stress load, surface details, interface area, packed size, printability, and assembling. Based on this analysis, we then formulate level set methods to improve the qualities of the partitioning according to the metrics. These methods are integrated into an automatic system, which repetitively and locally optimizes the partitioning. Given the optimized partitioning result, we further provide a container structure modeling algorithm to facilitate the packing process of the printed pieces. Our experiment shows that the system can generate quality partitioning of various 3D models for space saving and fast production purposes.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">15-SIGA-LevelSetPrintable</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yao, Miaojun and Chen, Zhili and Luo, Linjie and Wang, Rui and Wang, Huamin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Level-Set-Based Partitioning and Packing Optimization of a Printable Model}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="database-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/database-thumbnail.jpg"><div id="database-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('database-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="database-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("database-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('database-thumbnailjpg');
      var modalImg = document.getElementById("database-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="15-SIG-Hairbooth" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Single-View Hair Modeling Using A Hairstyle Database</div>
    <!-- Author -->
    <div class="author">
      

      Liwen Hu, Chongyang Ma, <em>Linjie Luo</em>, and Hao Li</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH)</em>, Aug 2015
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/15-SIG-SingleViewHairModelingUsingAHairstyleDatabase.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Hair digitalization has been one of the most critical and challenging tasks necessary to create virtual avatars. Most existing hair modeling techniques require either expensive capture devices or tedious manual effort. In this paper, we present a data-driven approach to create a complex 3D hairstyle from the single view of a photograph. We first build a database of 343 manually created 3D example hairstyles from some online repositories. Given a reference photo of the target hairstyle and a few user strokes as guidance, we automatically search for several best matching examples from the database and consistently combine them into a single hairstyle as the large-scale structure of the modeling output. Then we synthesize the final strands by ensuring the projected 2D similarity with the reference photo, the 3D physical plausibility of each individual strand as well as the local orientational coherency between neighboring strands simultaneously. We demonstrate the effectiveness and robustness of our method through a variety of hairstyles and compare with state-of-the-art hair modeling algorithms.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">15-SIG-Hairbooth</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Liwen and Ma, Chongyang and Luo, Linjie and Li, Hao}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Single-View Hair Modeling Using A Hairstyle Database}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2014</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="braids-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/braids-thumbnail.jpg"><div id="braids-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('braids-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="braids-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("braids-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('braids-thumbnailjpg');
      var modalImg = document.getElementById("braids-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="14-SIGA-Braid" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Capturing Braided Hairstyles</div>
    <!-- Author -->
    <div class="author">
      

      Liwen Hu, Chongyang Ma, <em>Linjie Luo</em>, Li-Yi Wei, and Hao Li</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Dec 2014
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/14-SIGA-CapturingBraidedHairstyles.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>From fishtail to princess braids, these intricately woven structures define an important and popular class of hairstyle, frequently used for digital characters in computer graphics. In addition to the challenges created by the infinite range of styles, existing modeling and capture techniques are particularly constrained by the geometric and topological complexities. We propose a data-driven method to automatically reconstruct braided hairstyles from input data obtained from a single consumer RGB-D camera. Our approach covers the large variation of repetitive braid structures using a family of compact procedural braid models. From these models, we produce a database of braid patches and use a robust random sampling approach for data fitting. We then recover the input braid structures using a multi-label optimization algorithm and synthesize the intertwining hair strands of the braids. We demonstrate that a minimal capture equipment is sufficient to effectively capture a wide range of complex braids with distinct shapes and structures.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">14-SIGA-Braid</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Liwen and Ma, Chongyang and Luo, Linjie and Wei, Li-Yi and Li, Hao}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Capturing Braided Hairstyles}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="simulated-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/simulated-thumbnail.jpg"><div id="simulated-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('simulated-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="simulated-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("simulated-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('simulated-thumbnailjpg');
      var modalImg = document.getElementById("simulated-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="14-SIG-SimulatedHair" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Robust Hair Capture Using Simulated Examples</div>
    <!-- Author -->
    <div class="author">
      

      Liwen Hu, Chongyang Ma, <em>Linjie Luo</em>, and Hao Li</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH)</em>, Jul 2014
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/14-SIG-RobustHairCaptureUsingSimulatedExamples.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We introduce a data-driven hair capture framework based on example strands generated through hair simulation. Our method can robustly reconstruct faithful 3D hair models from unprocessed input point clouds with large amounts of outliers. Current state-of-the-art techniques use geometrically-inspired heuristics to derive global hair strand structures, which can yield implausible hair strands for hairstyles involving large occlusions, multiple layers, or wisps of varying lengths. We address this problem using a voting-based fitting algorithm to discover structurally plausible configurations among the locally grown hair segments from a database of simulated examples. To generate these examples, we exhaustively sample the simulation configurations within the feasible parameter space constrained by the current input hairstyle. The number of necessary simulations can be further reduced by leveraging symmetry and constrained initial conditions. The final hairstyle can then be structurally represented by a limited number of examples. To handle constrained hairstyles such as a ponytail of which realistic simulations are more difficult, we allow the user to sketch a few strokes to generate strand examples through an intuitive interface. Our approach focuses on robustness and generality. Since our method is structurally plausible by construction, we ensure an improved control during hair digitization and avoid implausible hair synthesis for a wide range of hairstyles.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">14-SIG-SimulatedHair</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Liwen and Ma, Chongyang and Luo, Linjie and Li, Hao}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust Hair Capture Using Simulated Examples}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2013</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="portrait-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/portrait-thumbnail.jpg"><div id="portrait-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('portrait-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="portrait-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("portrait-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('portrait-thumbnailjpg');
      var modalImg = document.getElementById("portrait-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="13-SIGA-3DSelfPortraits" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">3D Self-Portraits</div>
    <!-- Author -->
    <div class="author">
      

      Hao Li, Etienne Vouga, Anton Gudym, <em>Linjie Luo</em>, Jonathan T. Barron, and Gleb Gusev</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Dec 2013
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/13-SIGA-3DSelfPortraits.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We develop an automatic pipeline that allows ordinary users to capture complete and fully textured 3D models of themselves in minutes, using only a single Kinect sensor, in the uncontrolled lighting environment of their own home. Our method requires neither a turntable nor a second operator, and is robust to the small deformations and changes of pose that inevitably arise during scanning. After the users rotate themselves with the same pose for a few scans from different views, our system stitches together the captured scans using multi-view non-rigid registration, and produces watertight final models. To ensure consistent texturing, we recover the underlying albedo from each scanned texture and generate seamless global textures using Poisson blending. Despite the minimal requirements we place on the hardware and users, our method is suitable for full body capture of challenging scenes that cannot be handled well using previous methods, such as those involving loose clothing, complex poses, and props.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">13-SIGA-3DSelfPortraits</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Hao and Vouga, Etienne and Gudym, Anton and Luo, Linjie and Barron, Jonathan T. and Gusev, Gleb}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{3D Self-Portraits}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="structure-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/structure-thumbnail.jpg"><div id="structure-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('structure-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="structure-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("structure-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('structure-thumbnailjpg');
      var modalImg = document.getElementById("structure-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="13-SIG-StructureHair" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Structure-Aware Hair Capture</div>
    <!-- Author -->
    <div class="author">
      

      <em>Linjie Luo</em>, Hao Li, and Szymon Rusinkiewicz</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH)</em>, Jul 2013
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/14-SIG-StructureAwareHairCapture.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Existing hair capture systems fail to produce strands that reflect the structures of real-world hairstyles. We introduce a system that reconstructs coherent and plausible wisps aware of the underlying hair structures from a set of still images without any special lighting. Our system first discovers locally coherent wisp structures in the reconstructed point cloud and the 3D orientation field, and then uses a novel graph data structure to reason about both the connectivity and directions of the local wisp structures in a global optimization. The wisps are then completed and used to synthesize hair strands which are robust against occlusion and missing data and plausible for animation and simulation. We show reconstruction results for a variety of complex hairstyles including curly, wispy, and messy hair.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">13-SIG-StructureHair</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Linjie and Li, Hao and Rusinkiewicz, Szymon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Structure-Aware Hair Capture}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="widebaseline-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/widebaseline-thumbnail.jpg"><div id="widebaseline-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('widebaseline-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="widebaseline-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("widebaseline-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('widebaseline-thumbnailjpg');
      var modalImg = document.getElementById("widebaseline-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="13-CVPR-WideHair" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Wide-baseline Hair Capture using Strand-based Refinement</div>
    <!-- Author -->
    <div class="author">
      

      <em>Linjie Luo</em>, Cha Zhang, Zhengyou Zhang, and Szymon Rusinkiewicz</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2013
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/13-CVPR-WideBaselineHairCaptureUsingStrandBasedRefinement.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands are first extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of  3mm. We also show real-world examples to demonstrate the capability to capture full-head hair styles as well as hair in motion with as few as 8 cameras.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">13-CVPR-WideHair</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Linjie and Zhang, Cha and Zhang, Zhengyou and Rusinkiewicz, Szymon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Wide-baseline Hair Capture using Strand-based Refinement}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2012</h2>
        <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="chopper-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/chopper-thumbnail.jpg"><div id="chopper-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('chopper-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="chopper-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("chopper-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('chopper-thumbnailjpg');
      var modalImg = document.getElementById("chopper-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="12-SIGA-Chopper" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Chopper: Partitioning Models into 3D-Printable Parts</div>
    <!-- Author -->
    <div class="author">
      

      <em>Linjie Luo</em>, Ilya Baran, Szymon Rusinkiewicz, and Wojciech Matusik</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</em>, Dec 2012
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/12-SIGA-ChopperPartitioningModelsInto3DPrintableParts.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>3D printing technology is rapidly maturing and becoming ubiquitous. One of the remaining obstacles to wide-scale adoption is that the object to be printed must fit into the working volume of the 3D printer. We propose a framework, called Chopper, to decompose a large 3D object into smaller parts so that each part fits into the printing volume. These parts can then be assembled to form the original object. We formulate a number of desirable criteria for the partition, including assemblability, having few components, unobtrusiveness of the seams, and structural soundness. Chopper optimizes these criteria and generates a partition either automatically or with user guidance. Our prototype outputs the final decomposed parts with customized connectors on the interfaces. We demonstrate the effectiveness of Chopper on a variety of non-trivial real-world objects.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">12-SIGA-Chopper</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Linjie and Baran, Ilya and Rusinkiewicz, Szymon and Matusik, Wojciech}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Chopper: Partitioning Models into {3D}-Printable Parts}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics (Proc. SIGGRAPH Asia)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="orientation-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/orientation-thumbnail.jpg"><div id="orientation-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('orientation-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="orientation-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("orientation-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('orientation-thumbnailjpg');
      var modalImg = document.getElementById("orientation-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="12-CVPR-HairCap" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Multi-View Hair Capture Using Orientation Fields</div>
    <!-- Author -->
    <div class="author">
      

      <em>Linjie Luo</em>, Hao Li, Sylvain Paris, Thibaut Weise, Mark Pauly, and Szymon Rusinkiewicz</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2012
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/12-CVPR-HairCap.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Reconstructing realistic 3D hair geometry is challenging due to omnipresent occlusions, complex discontinuities and specular appearance. To address these challenges, we propose a multi-view hair reconstruction algorithm based on orientation fields with structure-aware aggregation. Our key insight is that while hair’s color appearance is view-dependent, the response to oriented filters that captures the local hair orientation is more stable. We apply the structure-aware aggregation to the MRF matching energy to enforce the structural continuities implied from the local hair orientations. Multiple depth maps from the MRF optimization are then fused into a globally consistent hair geometry with a template refinement procedure. Compared to the state-of-the-art color-based methods, our method faithfully reconstructs detailed hair structures. We demonstrate the results for a number of hair styles, ranging from straight to curly, and show that our framework is suitable for capturing hair in motion.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">12-CVPR-HairCap</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Linjie and Li, Hao and Paris, Sylvain and Weise, Thibaut and Pauly, Mark and Rusinkiewicz, Szymon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-View Hair Capture Using Orientation Fields}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="shape-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/shape-thumbnail.jpg"><div id="shape-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('shape-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="shape-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("shape-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('shape-thumbnailjpg');
      var modalImg = document.getElementById("shape-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="12-TOG-DynamicRecon" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Temporally Coherent Completion of Dynamic Shapes</div>
    <!-- Author -->
    <div class="author">
      

      Hao Li, <em>Linjie Luo</em>, Daniel Vlasic, Pieter Peers, Jovan Popovi’c, Mark Pauly, and Szymon Rusinkiewicz</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Graphics</em>, Jan 2012
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/12-TOG-TemporallyCoherentCompletionDynamicShapes.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present a novel shape completion technique for creating temporally coherent watertight surfaces from real-time captured dynamic performances. Because of occlusions and low surface albedo, scanned mesh sequences typically exhibit large holes that persist over extended periods of time. Most conventional dynamic shape reconstruction techniques rely on template models or assume slow deformations in the input data. Our framework sidesteps these requirements and directly initializes shape completion with topology derived from the visual hull. To seal the holes with patches that are consistent with the subject’s motion, we first minimize surface bending energies in each frame to ensure smooth transitions across hole boundaries. Temporally coherent dynamics of surface patches are obtained by unwarping all frames within a time window using accurate interframe correspondences. Aggregated surface samples are then filtered with a temporal visibility kernel that maximizes the use of nonoccluded surfaces. A key benefit of our shape completion strategy is that it does not rely on long-range correspondences or a template model. Consequently, our method does not suffer error accumulation typically introduced by noise, large deformations, and drastic topological changes. We illustrate the effectiveness of our method on several high-resolution scans of human performances captured with a state-of-the-art multiview 3D acquisition system.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">12-TOG-DynamicRecon</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Hao and Luo, Linjie and Vlasic, Daniel and Peers, Pieter and Popovi{'c}, Jovan and Pauly, Mark and Rusinkiewicz, Szymon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Temporally Coherent Completion of Dynamic Shapes}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Graphics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li>
</ol>
        
        <h2 class="year">2009</h2>
        <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="laplace-thumbnailjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/laplace-thumbnail.jpg"><div id="laplace-thumbnailjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('laplace-thumbnailjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="laplace-thumbnailjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("laplace-thumbnailjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('laplace-thumbnailjpg');
      var modalImg = document.getElementById("laplace-thumbnailjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="09-SGP-ScreenedPoissonRecon" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Estimating the Laplace-Beltrami Operator by Restricting 3D Functions</div>
    <!-- Author -->
    <div class="author">
      

      Ming Chuang, <em>Linjie Luo</em>, Benedict J. Brown, Szymon Rusinkiewicz, and Michael Kazhdan</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Symposium on Geometry Processing</em>, Jul 2009
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      <a href="/assets/pdf/09-SGP-EstimatingLaplaceBeltramiOperatorByRestricting3DFunctions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present a novel approach for computing and solving the Poisson equation over the surface of a mesh. As in previous approaches, we define the Laplace-Beltrami operator by considering the derivatives of functions defined on the mesh. However, in this work, we explore a choice of functions that is decoupled from the tessellation. Specifically, we use basis functions (second-order tensor-product B-splines) defined over 3D space, and then restrict them to the surface. We show that in addition to being invariant to mesh topology, this definition of the Laplace-Beltrami operator allows a natural multiresolution structure on the function space that is independent of the mesh structure, enabling the use of a simple multigrid implementation for solving the Poisson equation.</p>
    </div>
<!-- Hidden bibtex block -->
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">09-SGP-ScreenedPoissonRecon</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chuang, Ming and Luo, Linjie and Brown, Benedict J. and Rusinkiewicz, Szymon and Kazhdan, Michael}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Estimating the {Laplace}-{Beltrami} Operator by Restricting {3D} Functions}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Symposium on Geometry Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2009}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
  </div>
</div>
</li></ol>
        

      </div>
    </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Linjie  Luo. Last updated: November 21, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

  </body>
</html>
